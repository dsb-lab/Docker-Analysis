{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as scp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import umap\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from mySCfunctions import *\n",
    "from silent import *\n",
    "sys.path.append('./')\n",
    "\n",
    "import Parameters as par\n",
    "from importlib import reload\n",
    "reload(par);\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_genes_list = \\\n",
    "    ['Mcm5', 'Pcna', 'Tyms', 'Fen1', 'Mcm2', 'Mcm4', 'Rrm1', 'Ung', 'Gins2',\n",
    "     'Mcm6', 'Cdca7', 'Dtl', 'Prim1', 'Uhrf1', 'Mlf1ip', 'Hells', 'Rfc2',\n",
    "     'Rpa2', 'Nasp', 'Rad51ap1', 'Gmnn', 'Wdr76', 'Slbp', 'Ccne2', 'Ubr7',\n",
    "     'Pold3', 'Msh2', 'Atad2', 'Rad51', 'Rrm2', 'Cdc45', 'Cdc6', 'Exo1', 'Tipin',\n",
    "     'Dscc1', 'Blm', 'Casp8ap2', 'Usp1', 'Clspn', 'Pola1', 'Chaf1b', 'Brip1', 'E2f8']\n",
    "g2m_genes_list = \\\n",
    "    ['Hmgb2', 'Cdk1', 'Nusap1', 'Ube2c', 'Birc5', 'Tpx2', 'Top2a', 'Ndc80',\n",
    "     'Cks2', 'Nuf2', 'Cks1b', 'Mki67', 'Tmpo', 'Cenpf', 'Tacc3', 'Fam64a',\n",
    "     'Smc4', 'Ccnb2', 'Ckap2l', 'Ckap2', 'Aurkb', 'Bub1', 'Kif11', 'Anp32e',\n",
    "     'Tubb4b', 'Gtse1', 'Kif20b', 'Hjurp', 'Cdca3', 'Hn1', 'Cdc20', 'Ttk',\n",
    "     'Cdc25c', 'Kif2c', 'Rangap1', 'Ncapd2', 'Dlgap5', 'Cdca2', 'Cdca8',\n",
    "     'Ect2', 'Kif23', 'Hmmr', 'Aurka', 'Psrc1', 'Anln', 'Lbr', 'Ckap5',\n",
    "     'Cenpe', 'Ctcf', 'Nek2', 'G2e3', 'Gas2l3', 'Cbx5', 'Cenpa']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data\n",
    "\n",
    "We upload the data with the selection of HVGs and log transformed. The log transformation of the data will make that the PCs do have a less dominant contribution among the genes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = scp.read(\"ManipulatedData/\"+par.METRIC+\"/Doublets_removed.h5ad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A la Marioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample\n",
      "E8.5\n",
      "Performing cosine normalization...\n",
      "Starting MNN correct iteration. Reference batch: 0\n",
      "Step 1 of 3: processing batch 1\n",
      "  Looking for MNNs...\n",
      "  Computing correction vectors...\n",
      "  Adjusting variance...\n",
      "  Applying correction...\n",
      "Step 2 of 3: processing batch 2\n",
      "  Looking for MNNs...\n",
      "  Computing correction vectors...\n",
      "  Adjusting variance...\n",
      "  Applying correction...\n",
      "Step 3 of 3: processing batch 3\n",
      "  Looking for MNNs...\n",
      "  Computing correction vectors...\n",
      "  Adjusting variance...\n",
      "  Applying correction...\n",
      "MNN correction complete. Gathering output...\n",
      "Packing AnnData object...\n",
      "Done.\n",
      "E8.25\n",
      "Performing cosine normalization...\n",
      "Starting MNN correct iteration. Reference batch: 0\n",
      "Step 1 of 2: processing batch 1\n",
      "  Looking for MNNs...\n",
      "  Computing correction vectors...\n",
      "  Adjusting variance...\n",
      "  Applying correction...\n",
      "Step 2 of 2: processing batch 2\n",
      "  Looking for MNNs...\n",
      "  Computing correction vectors...\n",
      "  Adjusting variance...\n",
      "  Applying correction...\n",
      "MNN correction complete. Gathering output...\n",
      "Packing AnnData object...\n",
      "Done.\n",
      "E8.0\n",
      "Performing cosine normalization...\n",
      "Starting MNN correct iteration. Reference batch: 0\n",
      "Step 1 of 3: processing batch 1\n",
      "  Looking for MNNs...\n",
      "  Computing correction vectors...\n",
      "  Adjusting variance...\n",
      "  Applying correction...\n",
      "Step 2 of 3: processing batch 2\n",
      "  Looking for MNNs...\n",
      "  Computing correction vectors...\n",
      "  Adjusting variance...\n",
      "  Applying correction...\n",
      "Step 3 of 3: processing batch 3\n",
      "  Looking for MNNs...\n",
      "  Computing correction vectors...\n",
      "  Adjusting variance...\n",
      "  Applying correction...\n",
      "MNN correction complete. Gathering output...\n",
      "Packing AnnData object...\n",
      "Done.\n",
      "E7.75\n",
      "Performing cosine normalization...\n",
      "Starting MNN correct iteration. Reference batch: 0\n",
      "Step 1 of 3: processing batch 1\n",
      "  Looking for MNNs...\n",
      "  Computing correction vectors...\n",
      "  Adjusting variance...\n",
      "  Applying correction...\n",
      "Step 2 of 3: processing batch 2\n",
      "  Looking for MNNs...\n",
      "  Computing correction vectors...\n",
      "  Adjusting variance...\n",
      "  Applying correction...\n",
      "Step 3 of 3: processing batch 3\n",
      "  Looking for MNNs...\n",
      "  Computing correction vectors...\n",
      "  Adjusting variance...\n",
      "  Applying correction...\n",
      "MNN correction complete. Gathering output...\n",
      "Packing AnnData object...\n",
      "Done.\n",
      "E7.5\n",
      "Performing cosine normalization...\n",
      "Starting MNN correct iteration. Reference batch: 0\n",
      "Step 1 of 5: processing batch 1\n",
      "  Looking for MNNs...\n",
      "  Computing correction vectors...\n",
      "  Adjusting variance...\n",
      "  Applying correction...\n",
      "Step 2 of 5: processing batch 2\n",
      "  Looking for MNNs...\n",
      "  Computing correction vectors...\n",
      "  Adjusting variance...\n",
      "  Applying correction...\n",
      "Step 3 of 5: processing batch 3\n",
      "  Looking for MNNs...\n",
      "  Computing correction vectors...\n",
      "  Adjusting variance...\n",
      "  Applying correction...\n",
      "Step 4 of 5: processing batch 4\n",
      "  Looking for MNNs...\n",
      "  Computing correction vectors...\n",
      "  Adjusting variance...\n",
      "  Applying correction...\n",
      "Step 5 of 5: processing batch 5\n",
      "  Looking for MNNs...\n",
      "  Computing correction vectors...\n",
      "  Adjusting variance...\n",
      "  Applying correction...\n",
      "MNN correction complete. Gathering output...\n",
      "Packing AnnData object...\n",
      "Done.\n",
      "E7.25\n",
      "Performing cosine normalization...\n",
      "Starting MNN correct iteration. Reference batch: 0\n",
      "Step 1 of 2: processing batch 1\n",
      "  Looking for MNNs...\n",
      "  Computing correction vectors...\n",
      "  Adjusting variance...\n",
      "  Applying correction...\n",
      "Step 2 of 2: processing batch 2\n",
      "  Looking for MNNs...\n",
      "  Computing correction vectors...\n",
      "  Adjusting variance...\n",
      "  Applying correction...\n",
      "MNN correction complete. Gathering output...\n",
      "Packing AnnData object...\n",
      "Done.\n",
      "E7.0\n",
      "Performing cosine normalization...\n",
      "Starting MNN correct iteration. Reference batch: 0\n",
      "Step 1 of 5: processing batch 1\n",
      "  Looking for MNNs...\n",
      "  Computing correction vectors...\n",
      "  Adjusting variance...\n",
      "  Applying correction...\n",
      "Step 2 of 5: processing batch 2\n",
      "  Looking for MNNs...\n",
      "  Computing correction vectors...\n",
      "  Adjusting variance...\n",
      "  Applying correction...\n",
      "Step 3 of 5: processing batch 3\n",
      "  Looking for MNNs...\n",
      "  Computing correction vectors...\n",
      "  Adjusting variance...\n",
      "  Applying correction...\n",
      "Step 4 of 5: processing batch 4\n",
      "  Looking for MNNs...\n",
      "  Computing correction vectors...\n",
      "  Adjusting variance...\n",
      "  Applying correction...\n",
      "Step 5 of 5: processing batch 5\n",
      "  Looking for MNNs...\n",
      "  Computing correction vectors...\n",
      "  Adjusting variance...\n",
      "  Applying correction...\n",
      "MNN correction complete. Gathering output...\n",
      "Packing AnnData object...\n",
      "Done.\n",
      "E6.75\n",
      "E6.5\n",
      "Performing cosine normalization...\n",
      "Starting MNN correct iteration. Reference batch: 0\n",
      "Step 1 of 2: processing batch 1\n",
      "  Looking for MNNs...\n",
      "  Computing correction vectors...\n",
      "  Adjusting variance...\n",
      "  Applying correction...\n",
      "Step 2 of 2: processing batch 2\n",
      "  Looking for MNNs...\n",
      "  Computing correction vectors...\n",
      "  Adjusting variance...\n",
      "  Applying correction...\n",
      "MNN correction complete. Gathering output...\n",
      "Packing AnnData object...\n",
      "Done.\n",
      "Performing cosine normalization...\n",
      "Starting MNN correct iteration. Reference batch: 0\n",
      "Step 1 of 8: processing batch 1\n",
      "  Looking for MNNs...\n",
      "  Computing correction vectors...\n",
      "  Adjusting variance...\n",
      "  Applying correction...\n",
      "Step 2 of 8: processing batch 2\n",
      "  Looking for MNNs...\n",
      "  Computing correction vectors...\n",
      "  Adjusting variance...\n",
      "  Applying correction...\n",
      "Step 3 of 8: processing batch 3\n",
      "  Looking for MNNs...\n",
      "  Computing correction vectors...\n",
      "  Adjusting variance...\n",
      "  Applying correction...\n",
      "Step 4 of 8: processing batch 4\n",
      "  Looking for MNNs...\n",
      "  Computing correction vectors...\n",
      "  Adjusting variance...\n",
      "  Applying correction...\n",
      "Step 5 of 8: processing batch 5\n",
      "  Looking for MNNs...\n",
      "  Computing correction vectors...\n",
      "  Adjusting variance...\n",
      "  Applying correction...\n",
      "Step 6 of 8: processing batch 6\n",
      "  Looking for MNNs...\n",
      "  Computing correction vectors...\n",
      "  Adjusting variance...\n",
      "  Applying correction...\n",
      "Step 7 of 8: processing batch 7\n",
      "  Looking for MNNs...\n",
      "  Computing correction vectors...\n",
      "  Adjusting variance...\n",
      "  Applying correction...\n",
      "Step 8 of 8: processing batch 8\n",
      "  Looking for MNNs...\n",
      "  Computing correction vectors...\n",
      "  Adjusting variance...\n",
      "  Applying correction...\n",
      "MNN correction complete. Gathering output...\n",
      "Packing AnnData object...\n",
      "Done.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Value passed for key 'X_MNN' is of incorrect shape. Values of obsm must match dimensions (0,) of parent. Value had shape (111622, 50) while it should have had (119836,).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-86b90c9695e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sample\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sample\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"str\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mmnn_correct_twice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"sample\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"stage\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"E8.5\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"E8.25\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"E8.0\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"E7.75\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"E7.5\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"E7.25\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"E7.0\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"E6.75\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"E6.5\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./ManipulatedData/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mpar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMETRIC\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/tables/full_data_integration_Marioni_pca.npy\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"louvain\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/PhD/Projects/Somitogenesis/Analysis/Version 5/../mySCfunctions.py\u001b[0m in \u001b[0;36mmnn_correct_twice\u001b[0;34m(adata, key1, key2, order, key_obsm, key_added, **kwargs)\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m     \u001b[0madata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobsm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey_added\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.python-virtual-environments/scRNAseq/lib/python3.9/site-packages/anndata/_core/aligned_mapping.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.python-virtual-environments/scRNAseq/lib/python3.9/site-packages/anndata/_core/aligned_mapping.py\u001b[0m in \u001b[0;36m_validate_value\u001b[0;34m(self, val, key)\u001b[0m\n\u001b[1;32m    213\u001b[0m                 \u001b[0;34mf\"value.index does not match parent’s axis {self.axes[0]} names\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m             )\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.python-virtual-environments/scRNAseq/lib/python3.9/site-packages/anndata/_core/aligned_mapping.py\u001b[0m in \u001b[0;36m_validate_value\u001b[0;34m(self, val, key)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0mright_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m     53\u001b[0m                     \u001b[0;34mf\"Value passed for key {key!r} is of incorrect shape. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                     \u001b[0;34mf\"Values of {self.attrname} must match dimensions \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Value passed for key 'X_MNN' is of incorrect shape. Values of obsm must match dimensions (0,) of parent. Value had shape (111622, 50) while it should have had (119836,)."
     ]
    }
   ],
   "source": [
    "scp.pp.log1p(a)\n",
    "scp.pp.highly_variable_genes(a,flavor=\"seurat\")\n",
    "for i in s_genes_list:\n",
    "    a.var.loc[a.var[\"Gene\"]==i,\"highly_variable\"] = False\n",
    "    \n",
    "for i in g2m_genes_list:\n",
    "    a.var.loc[a.var[\"Gene\"]==i,\"highly_variable\"] = False\n",
    "    \n",
    "scp.pp.pca(a,n_comps=par.N_PCS,use_highly_variable=par.USE_HVGs)\n",
    "\n",
    "a.obs[\"sample\"] = a.obs[\"sample\"].astype(\"str\")\n",
    "mnn_correct_twice(a,\"sample\",\"stage\",[\"E8.5\",\"E8.25\",\"E8.0\",\"E7.75\",\"E7.5\",\"E7.25\",\"E7.0\",\"E6.75\",\"E6.5\"])\n",
    "\n",
    "np.save(\"./ManipulatedData/\"+par.METRIC+\"/tables/full_data_integration_Marioni_pca.npy\",a.obs.loc[:,\"louvain\"].values.astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.obsm[\"X_pca_MNN\"] = np.load(\"./ManipulatedData/\"+par.METRIC+\"/tables/full_data_integration_Marioni_pca.npy\")\n",
    "\n",
    "scp.pp.neighbors(a,use_rep=\"X_MNN\",n_pcs=par.N_PCS,knn=par.N_NEIGBOURS)\n",
    "\n",
    "scp.tl.umap(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1,figsize=[20,20])\n",
    "\n",
    "sb.scatterplot(a.obsm[\"X_umap\"][:,0],a.obsm[\"X_umap\"][:,1],a.obs.loc[:,\"stage\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A la Klein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA, TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = (a.obs[\"stage\"]==\"E8.5\") + (a.obs[\"stage\"]==\"E8.25\") \n",
    "b = a[l,:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scp.pp.log1p(b)\n",
    "scp.pp.highly_variable_genes(b,flavor=par.HVG_METHOD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = b[b.obs[\"stage\"]==\"E8.5\",b.var[\"highly_variable\"]].X.copy()\n",
    "X2 = b[b.obs[\"stage\"]==\"E8.25\",b.var[\"highly_variable\"]].X.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_norm = preprocessing.MaxAbsScaler().fit_transform(X)\n",
    "X2_norm = preprocessing.MaxAbsScaler().fit_transform(X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TruncatedSVD(n_components=par.N_PCS).fit(X_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca = model.fit_transform(X_norm)\n",
    "X2_pca = model.fit_transform(X2_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.obsm[\"X_pca\"] = np.zeros([b.shape[0],par.N_PCS])\n",
    "b.obsm[\"X_pca\"][b.obs[\"stage\"]==\"E8.5\",:] = X_pca\n",
    "b.obsm[\"X_pca\"][b.obs[\"stage\"]==\"E8.25\",:] = X2_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<32588x32588 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 456232 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Make nearest neighbours (1)\n",
    "scp.pp.neighbors(b,metric=par.METRIC,knn=200,use_rep=\"X_pca\",n_pcs=par.N_PCS)\n",
    "d = b.obsp[\"distances\"].copy()\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<32588x32588 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 455744 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cutoff local (2)\n",
    "maxDistance = 3*(d.power(-1).max(axis=1).power(-1)).toarray()\n",
    "loc_remove = sp.sparse.csc_matrix((d > maxDistance)).nonzero()\n",
    "d[loc_remove] = 0\n",
    "d.eliminate_zeros()\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<32588x32588 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 395284 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cutoff global (3)\n",
    "s1 = b.obs[\"stage\"]==\"E8.5\"\n",
    "s2 = b.obs[\"stage\"]==\"E8.25\"\n",
    "\n",
    "#Remove between samples\n",
    "mean_across_samples = (d[s1][:,s2].sum()+d[s2][:,s1].sum())/(d[s1][:,s2].data.shape[0]+d[s2][:,s1].data.shape[0])\n",
    "\n",
    "loc = (np.array([i for i,j in zip(d.nonzero()[0],d.nonzero()[1]) if s1[i] and s2[j]])\n",
    "       ,np.array([j for i,j in zip(d.nonzero()[0],d.nonzero()[1]) if s1[i] and s2[j]]))\n",
    "\n",
    "v = d[loc]\n",
    "v[d[loc] > mean_across_samples] = 0\n",
    "d[loc] = v\n",
    "\n",
    "loc = (np.array([i for i,j in zip(d.nonzero()[0],d.nonzero()[1]) if s2[i] and s1[j]])\n",
    "       ,np.array([j for i,j in zip(d.nonzero()[0],d.nonzero()[1]) if s2[i] and s1[j]]))\n",
    "\n",
    "v = d[loc]\n",
    "v[d[loc] > mean_across_samples] = 0\n",
    "d[loc] = v\n",
    "\n",
    "#Remove within sample\n",
    "meanDistance = d[s1][:,s1].sum()/d[s1][:,s1].data.shape[0]\n",
    "stdDistance = np.sqrt(d[s1][:,s1].power(2).sum()/d[s1][:,s1].data.shape[0]-np.power(d[s1][:,s1].sum()/d[s1][:,s1].data.shape[0],2))\n",
    "maxDistance_s1 = (meanDistance+stdDistance)\n",
    "\n",
    "meanDistance = d[s2][:,s2].sum()/d[s2][:,s2].data.shape[0]\n",
    "stdDistance = np.sqrt(d[s2][:,s2].power(2).sum()/d[s2][:,s2].data.shape[0]-np.power(d[s2][:,s2].sum()/d[s2][:,s2].data.shape[0],2))\n",
    "maxDistance_s2 = (meanDistance+stdDistance)\n",
    "\n",
    "loc = (np.array([i for i,j in zip(d.nonzero()[0],d.nonzero()[1]) if s1[i] and s1[j]])\n",
    "       ,np.array([j for i,j in zip(d.nonzero()[0],d.nonzero()[1]) if s1[i] and s1[j]]))\n",
    "\n",
    "for i,j in zip(loc[0],loc[1]):\n",
    "    if d[i,j] > maxDistance_s1:\n",
    "        d[i,j] = 0\n",
    "        \n",
    "loc = (np.array([i for i,j in zip(d.nonzero()[0],d.nonzero()[1]) if s2[i] and s2[j]])\n",
    "       ,np.array([j for i,j in zip(d.nonzero()[0],d.nonzero()[1]) if s2[i] and s2[j]]))\n",
    "\n",
    "for i,j in zip(loc[0],loc[1]):\n",
    "    if d[i,j] > maxDistance_s2:\n",
    "        d[i,j] = 0\n",
    "        \n",
    "d.eliminate_zeros()\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<32588x32588 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 212660 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Limit to 20 MNN\n",
    "s1,s2 = d.nonzero()\n",
    "d[s1,s2] = d[s2,s1]\n",
    "d.eliminate_zeros()\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A la Klein modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA, TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2 = ['E8.5', 'E8.25', 'E8.0', 'E7.75', 'E7.5', 'E7.25', 'E7.0', 'E6.75']\n",
    "l1 = ['E8.25', 'E8.0', 'E7.75', 'E7.5', 'E7.25', 'E7.0', 'E6.75', 'E6.5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: unrecognised metric for type of neighbor calculation, switching to angular\n",
      "WARNING: unrecognised metric for type of neighbor calculation, switching to angular\n",
      "WARNING: unrecognised metric for type of neighbor calculation, switching to angular\n",
      "WARNING: unrecognised metric for type of neighbor calculation, switching to angular\n",
      "WARNING: unrecognised metric for type of neighbor calculation, switching to angular\n",
      "WARNING: unrecognised metric for type of neighbor calculation, switching to angular\n",
      "WARNING: unrecognised metric for type of neighbor calculation, switching to angular\n",
      "WARNING: unrecognised metric for type of neighbor calculation, switching to angular\n"
     ]
    }
   ],
   "source": [
    "data_edges = pd.DataFrame()\n",
    "\n",
    "for st1,st2 in zip(l1,l2):\n",
    "\n",
    "    #Extract datasets\n",
    "    l = (a.obs[\"stage\"]==st1) + (a.obs[\"stage\"]==st2) \n",
    "    b = a[l,:].copy()\n",
    "    \n",
    "    #Make list of cells for each stage\n",
    "    s1 = b.obs[\"stage\"]==st2\n",
    "    s2 = b.obs[\"stage\"]==st1\n",
    "    \n",
    "    #Log normalize\n",
    "    scp.pp.log1p(b)\n",
    "    \n",
    "    #Extract HVGs of time i+1\n",
    "    b2 = b[b.obs[\"stage\"]==st2,:].copy()\n",
    "    scp.pp.highly_variable_genes(b2,flavor=par.HVG_METHOD)\n",
    "    hvg = b2.var[\"highly_variable\"].values\n",
    "    del b2\n",
    "    \n",
    "    #Make PCs of time i+1 and project the space of i\n",
    "    X = b[b.obs[\"stage\"]==st1,hvg].X.copy()\n",
    "    X2 = b[b.obs[\"stage\"]==st2,hvg].X.copy()\n",
    "\n",
    "    X_norm = preprocessing.MaxAbsScaler().fit_transform(X)\n",
    "    X2_norm = preprocessing.MaxAbsScaler().fit_transform(X2)\n",
    "    \n",
    "    model = TruncatedSVD(n_components=par.N_PCS).fit(X2_norm) #Fit with the later time point\n",
    "    \n",
    "    X_pca = model.transform(X_norm)\n",
    "    X2_pca = model.transform(X2_norm)\n",
    "    \n",
    "    b.obsm[\"X_pca\"] = np.zeros([b.shape[0],par.N_PCS])\n",
    "    b.obsm[\"X_pca\"][b.obs[\"stage\"]==st1,:] = X_pca\n",
    "    b.obsm[\"X_pca\"][b.obs[\"stage\"]==st2,:] = X2_pca\n",
    "    \n",
    "    #Make neighbours between stages (1)\n",
    "    scp.external.pp.bbknn(b,\"stage\",metric=par.METRIC,neighbors_within_batch=10,use_rep=\"X_pca\",n_pcs=par.N_PCS)\n",
    "    d = b.obsp[\"distances\"].copy()\n",
    "    \n",
    "    #Cutoff local (2)\n",
    "    maxDistance = 3*(d.power(-1).max(axis=1).power(-1)).toarray()\n",
    "    loc_remove = sp.sparse.csc_matrix((d > maxDistance)).nonzero()\n",
    "    d[loc_remove] = 0\n",
    "    d.eliminate_zeros()\n",
    "    \n",
    "    #Cutoff global (3)\n",
    "        #Remove between samples\n",
    "    mean_across_samples = (d[s1][:,s2].sum()+d[s2][:,s1].sum())/(d[s1][:,s2].data.shape[0]+d[s2][:,s1].data.shape[0])\n",
    "\n",
    "    loc = (np.array([i for i,j in zip(d.nonzero()[0],d.nonzero()[1]) if s1[i] and s2[j]])\n",
    "           ,np.array([j for i,j in zip(d.nonzero()[0],d.nonzero()[1]) if s1[i] and s2[j]]))\n",
    "\n",
    "    v = d[loc]\n",
    "    v[d[loc] > mean_across_samples] = 0\n",
    "    d[loc] = v\n",
    "\n",
    "    loc = (np.array([i for i,j in zip(d.nonzero()[0],d.nonzero()[1]) if s2[i] and s1[j]])\n",
    "           ,np.array([j for i,j in zip(d.nonzero()[0],d.nonzero()[1]) if s2[i] and s1[j]]))\n",
    "\n",
    "    v = d[loc]\n",
    "    v[d[loc] > mean_across_samples] = 0\n",
    "    d[loc] = v\n",
    "\n",
    "        #Remove within sample\n",
    "    meanDistance = d[s1][:,s1].sum()/d[s1][:,s1].data.shape[0]\n",
    "    stdDistance = np.sqrt(d[s1][:,s1].power(2).sum()/d[s1][:,s1].data.shape[0]-np.power(d[s1][:,s1].sum()/d[s1][:,s1].data.shape[0],2))\n",
    "    maxDistance_s1 = (meanDistance+stdDistance)\n",
    "\n",
    "    meanDistance = d[s2][:,s2].sum()/d[s2][:,s2].data.shape[0]\n",
    "    stdDistance = np.sqrt(d[s2][:,s2].power(2).sum()/d[s2][:,s2].data.shape[0]-np.power(d[s2][:,s2].sum()/d[s2][:,s2].data.shape[0],2))\n",
    "    maxDistance_s2 = (meanDistance+stdDistance)\n",
    "\n",
    "    loc = (np.array([i for i,j in zip(d.nonzero()[0],d.nonzero()[1]) if s1[i] and s1[j]])\n",
    "           ,np.array([j for i,j in zip(d.nonzero()[0],d.nonzero()[1]) if s1[i] and s1[j]]))\n",
    "\n",
    "    for i,j in zip(loc[0],loc[1]):\n",
    "        if d[i,j] > maxDistance_s1:\n",
    "            d[i,j] = 0\n",
    "\n",
    "    loc = (np.array([i for i,j in zip(d.nonzero()[0],d.nonzero()[1]) if s2[i] and s2[j]])\n",
    "           ,np.array([j for i,j in zip(d.nonzero()[0],d.nonzero()[1]) if s2[i] and s2[j]]))\n",
    "\n",
    "    for i,j in zip(loc[0],loc[1]):\n",
    "        if d[i,j] > maxDistance_s2:\n",
    "            d[i,j] = 0\n",
    "\n",
    "    d.eliminate_zeros()\n",
    "    \n",
    "    #Remove excess of neighbours \n",
    "    #s1,s2 = d.nonzero()\n",
    "    #d[s1,s2] = d[s2,s1]\n",
    "    #d.eliminate_zeros()    \n",
    "    \n",
    "    data_edges_aux = pd.DataFrame()\n",
    "    data_edges_aux[\"source\"] = b.obs.index[d.nonzero()[0]]\n",
    "    data_edges_aux[\"target\"] = b.obs.index[d.nonzero()[1]]\n",
    "    data_edges_aux[\"value\"] = d.data\n",
    "    \n",
    "    data_edges = data_edges.append(data_edges_aux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_edges.to_csv(\"./ManipulatedData/\"+par.METRIC+\"/tables/Klein_network_edges.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "scRNAseq",
   "language": "python",
   "name": "scrnaseq"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "230px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
